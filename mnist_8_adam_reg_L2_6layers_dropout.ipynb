{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "mnist_8_adam_reg_L2_6layers_dropout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexandreblima/IC-THS/blob/master/mnist_8_adam_reg_L2_6layers_dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtlKNA_Ui0Cz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# arquivo: mnist_8_adam_reg_L2_6layers_dropout.py\n",
        "# Adaptação dos códigos do livro \"Deep Learning with TensorFlow2\n",
        "# and Keras: Regression, Convnets, GANs RNNs, NLP and more with\n",
        "# TensorFlow2 and the Keras API, de Antonio Gulli, Amita Kapoor \n",
        "# e Sujit Pal, 2a ed., Packt. \n",
        "# otimizador = Adam\n",
        "# regularização L2\n",
        "# código para TensorFlow 2.1.0 Spyder - Anaconda\n",
        "# autor: Alexandre B. de Lima (ABL)\n",
        "\n",
        "import tensorflow as tf\n",
        "#import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras import regularizers\n",
        "\n",
        "# #%% define célula\n",
        "\n",
        "# Parâmetros da rede e de treinamento\n",
        "epocas = 20    # define a duração do treinamento\n",
        "lote_tam = 64  # número de amostras que alimentarão a rede em uma dada época\n",
        "               # de treinamento (em inglês, lote = batch)\n",
        "verbose = 1\n",
        "n_classes = 10 # número de saídas = número de dígitos\n",
        "n_oculta = 128 \n",
        "val_split = 0.2 # a fração do número de amostras de treinamento reservadas para\n",
        "                # validação => 48.000 amostras para treino + 12.000 amostras para \n",
        "                # validação = 60.000 exemplos do conjunto de treinamento MNIST     \n",
        "dropout = 0.3   # técnica de regularização\n",
        "                # A idéia por trás dessa melhoria é que o descarte aleatório força a \n",
        "                # rede a aprender padrões redundantes que são úteis para uma melhor generalização\n",
        "\n",
        "#%\n",
        "\n",
        "# carrega a base de dados MNIST\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "reformat = 784 # é a dimensão do espaço de \"features\"\n",
        "               # ou seja, a camada de entrada possui 784 \"input units\"\n",
        "               # Uma entrada é um vetor coluna 784 x 1\n",
        " \n",
        "X_train = X_train.reshape(60000, reformat) \n",
        "X_test = X_test.reshape(10000, reformat) \n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# normalização em [0,1]\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.shape[0], 'amostras de treinamento')\n",
        "print(X_test.shape[0], 'amostras de teste')\n",
        "\n",
        "# Representação \"one-hot\" dos rótulos (labels) de treinamento e teste\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, n_classes)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, n_classes)\n",
        "\n",
        "# construção do modelo\n",
        "# A camada de saída é composta por 10 neurônios com função de ativação \"softmax\", que é uma \n",
        "# generalização da função sigmóide ou logística. \n",
        "\n",
        "# A complexidade de um modelo pode ser convenientemente representada como o número de pesos \n",
        "# diferentes de zero. Em outras palavras, se tivermos dois modelos M1 e M2 obtendo praticamente \n",
        "# o mesmo desempenho em termos de função de perda, devemos escolher o modelo mais simples, ou seja,\n",
        "# aquele que tenha o número mínimo de pesos diferentes de zero.\n",
        "# Regularização L2 (também conhecida como Ridge): a complexidade do modelo é expressa como a \n",
        "# soma dos quadrados dos pesos\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(n_oculta, input_shape=(reformat,), kernel_regularizer=regularizers.l2(0.001),\n",
        "                             name='dense_layer1', activation='relu'))\n",
        "model.add(keras.layers.Dropout(dropout))\n",
        "model.add(keras.layers.Dense(n_oculta, input_shape=(reformat,), kernel_regularizer=regularizers.l2(0.001),\n",
        "                             name='dense_layer2', activation='relu'))\n",
        "model.add(keras.layers.Dropout(dropout))\n",
        "model.add(keras.layers.Dense(n_classes, input_shape=(reformat,), name='dense_layer3', activation='softmax'))\n",
        "\n",
        "# sumário do modelo\n",
        "model.summary()\n",
        "\n",
        "#%\n",
        "# compilando o modelo\n",
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# RMSProp tende a convergir mais rápido que o SGD, vide:\n",
        "# https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a\n",
        "\n",
        "#%\n",
        "# treinamento do modelo no TensorFlow 2\n",
        "treino = model.fit(X_train, Y_train, batch_size=lote_tam, epochs=epocas, verbose=verbose, validation_split=val_split)\n",
        "# model.fit retorna o objeto \"treino\", que é do tipo \"History\".  \n",
        "\n",
        "treino_dic = treino.history \n",
        "# \"treino.history\" retorna um contêiner de dicionário (uma generalização do conceito de lista): tipo \"dict\" \n",
        "# Um dicionário contém pares de (chave, valor)\n",
        "treino_dic.keys()\n",
        "# chaves no tf 2.1.0: ['loss', 'accuracy', 'val_loss', 'val_accuracy']\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font = {'family': 'serif',\n",
        "        'color':  'darkred',\n",
        "        'weight': 'normal',\n",
        "        'size': 16,\n",
        "        }\n",
        "\n",
        "acc = treino.history['accuracy']\n",
        "val_acc = treino.history['val_accuracy']\n",
        "loss = treino.history['loss']\n",
        "val_loss = treino.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.clf()\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Acurácia de treinamento')\n",
        "plt.plot(epochs, val_acc, 'r', label='Acurácia de validação')\n",
        "plt.title('Curvas de aprendizado', fontdict=font)\n",
        "plt.xlabel('tempo (épocas)', fontdict=font)\n",
        "plt.ylabel('Acurácia', fontdict=font)\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Erro de treinamento')\n",
        "plt.plot(epochs, val_loss, 'r', label='Erro de validação')\n",
        "plt.title('Curvas de aprendizado ', fontdict=font)\n",
        "plt.xlabel('tempo (épocas)', fontdict=font)\n",
        "plt.ylabel('Erro', fontdict=font)\n",
        "plt.legend()\n",
        "\n",
        "# O gráfico de custo de treinamento e validação sugere que o \"overfitting\" ocorre após 9 épocas\n",
        "# ATENÇÃO: os \"scores\" de validação podem variar em função da partição do conjunto de treinamento que\n",
        "# você escolheu usar para validação e de treinamento: os \"scores\" de validação podem ter uma alta vari6ancia \n",
        "# em relação ao \"split\" de validação.\n",
        "\n",
        "#%\n",
        "# avaliação do modelo\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Acurácia do teste:', test_acc)\n",
        "# acurácio de teste: 97,49%\n",
        "\n",
        "#%\n",
        "# previsões\n",
        "predictions = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJSyhNSwi0C4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}