{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_10_adam_reg_BN_8layers_dropout",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPR0xkMCdP66TB3AZTKZlQ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexandreblima/IC-THS/blob/master/mnist_10_adam_reg_BN_8layers_dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4KUvM4YhWDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# arquivo: mnist_10_adam_reg_BN_8layers_dropout.py\n",
        "# Adaptação dos códigos do livro \"Deep Learning with TensorFlow2\n",
        "# and Keras: Regression, Convnets, GANs RNNs, NLP and more with\n",
        "# TensorFlow2 and the Keras API, de Antonio Gulli, Amita Kapoor \n",
        "# e Sujit Pal, 2a ed., Packt. \n",
        "# otimizador = Adam\n",
        "# regularização com BATCH NORMALIZATION\n",
        "# código para TensorFlow 2\n",
        "\n",
        "import tensorflow as tf\n",
        "#import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras import regularizers\n",
        "import datetime\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "%load_ext tensorboard\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/\n",
        "\n",
        "# Parâmetros da rede e de treinamento\n",
        "epocas = 20    # define a duração do treinamento\n",
        "lote_tam = 64  # número de amostras que alimentarão a rede em uma dada época\n",
        "               # de treinamento (em inglês, lote = batch)\n",
        "verbose = 1\n",
        "n_classes = 10 # número de saídas = número de dígitos\n",
        "n_oculta = 128 \n",
        "val_split = 0.2 # a fração do número de amostras de treinamento reservadas para\n",
        "                # validação => 48.000 amostras para treino + 12.000 amostras para \n",
        "                # validação = 60.000 exemplos do conjunto de treinamento MNIST     \n",
        "dropout = 0.3   # técnica de regularização\n",
        "                # A idéia por trás dessa melhoria é que o descarte aleatório força a \n",
        "                # rede a aprender padrões redundantes que são úteis para uma melhor generalização\n",
        "\n",
        "# carrega a base de dados MNIST\n",
        "#mnist = keras.datasets.mnist\n",
        "from keras.datasets import mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "reformat = 784 # é a dimensão do espaço de \"features\"\n",
        "               # ou seja, a camada de entrada possui 784 \"input units\"\n",
        "               # Uma entrada é um vetor coluna 784 x 1\n",
        " \n",
        "X_train = X_train.reshape(60000, reformat) \n",
        "X_test = X_test.reshape(10000, reformat) \n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# normalização em [0,1]\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.shape[0], 'amostras de treinamento')\n",
        "print(X_test.shape[0], 'amostras de teste')\n",
        "\n",
        "# Representação \"one-hot\" dos rótulos (labels) de treinamento e teste\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, n_classes)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, n_classes)\n",
        "\n",
        "# construção do modelo\n",
        "# A camada de saída é composta por 10 neurônios com função de ativação \"softmax\", que é uma \n",
        "# generalização da função sigmóide ou logística. \n",
        "\n",
        "# BATCH NORMALIZATION (BN)\n",
        "# Treinar redes neurais profundas é difícil. E fazê-las convergir em uma quantidade razoável de tempo pode ser complicado.\n",
        "# A normalização de lotes (BN) é uma técnica popular e eficaz que acelera consistentemente a convergência de redes profundas. \n",
        "# Juntamente com os blocos residuais (não foi implementado neste código), o BN tornou possível para os profissionais \n",
        "# treinar rotineiramente redes com mais de 100 camadas.\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(n_oculta, input_shape=(reformat,), kernel_regularizer=regularizers.l2(0.001),\n",
        "                             name='dense_layer1', activation='relu'))\n",
        "model.add(keras.layers.Dropout(dropout))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Dense(n_oculta, input_shape=(reformat,), kernel_regularizer=regularizers.l2(0.001),\n",
        "                             name='dense_layer2', activation='relu'))\n",
        "model.add(keras.layers.Dropout(dropout))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Dense(n_classes, input_shape=(reformat,), name='dense_layer3', activation='softmax'))\n",
        "\n",
        "# sumário do modelo\n",
        "model.summary()\n",
        "\n",
        "# compilando o modelo\n",
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# RMSProp tende a convergir mais rápido que o SGD, vide:\n",
        "# https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a\n",
        "\n",
        "# Tensorboard\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True)\n",
        "\n",
        "# treinamento do modelo no TensorFlow 2\n",
        "treino = model.fit(X_train, Y_train, batch_size=lote_tam, epochs=epocas, verbose=verbose, validation_split=val_split, callbacks=tensorboard_callback)\n",
        "# model.fit retorna o objeto \"treino\", que é do tipo \"History\".  \n",
        "\n",
        "treino_dic = treino.history \n",
        "# \"treino.history\" retorna um contêiner de dicionário (uma generalização do conceito de lista): tipo \"dict\" \n",
        "# Um dicionário contém pares de (chave, valor)\n",
        "treino_dic.keys()\n",
        "# chaves no tf 2.1.0: ['loss', 'accuracy', 'val_loss', 'val_accuracy']\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font = {'family': 'serif',\n",
        "        'color':  'darkred',\n",
        "        'weight': 'normal',\n",
        "        'size': 16,\n",
        "        }\n",
        "\n",
        "acc = treino.history['accuracy']\n",
        "val_acc = treino.history['val_accuracy']\n",
        "loss = treino.history['loss']\n",
        "val_loss = treino.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.clf()\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Acurácia de treinamento')\n",
        "plt.plot(epochs, val_acc, 'r', label='Acurácia de validação')\n",
        "plt.title('Curvas de aprendizado', fontdict=font)\n",
        "plt.xlabel('tempo (épocas)', fontdict=font)\n",
        "plt.ylabel('Acurácia', fontdict=font)\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Erro de treinamento')\n",
        "plt.plot(epochs, val_loss, 'r', label='Erro de validação')\n",
        "plt.title('Curvas de aprendizado ', fontdict=font)\n",
        "plt.xlabel('tempo (épocas)', fontdict=font)\n",
        "plt.ylabel('Erro', fontdict=font)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# O gráfico de custo de treinamento e validação sugere que o \"overfitting\" ocorre após 9 épocas\n",
        "# ATENÇÃO: os \"scores\" de validação podem variar em função da partição do conjunto de treinamento que\n",
        "# você escolheu usar para validação e de treinamento: os \"scores\" de validação podem ter uma alta vari6ancia \n",
        "# em relação ao \"split\" de validação.\n",
        "\n",
        "# avaliação do modelo\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Acurácia do teste:', test_acc)\n",
        "# acurácio de teste: 97,15% \n",
        "\n",
        "# previsões\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DduuwG1_1gZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}